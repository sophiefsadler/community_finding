{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Node Metric Calculations\n",
    "\n",
    "These metrics were the first 5 calculated for each node. They're calculated over the entire graph (i.e. community structure is irrelevant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_node_metrics(G):\n",
    "    node_degrees = dict(G.degree())\n",
    "    node_clustering_coefficients = nx.clustering(G)\n",
    "    node_betweenness = nx.betweenness_centrality(G)\n",
    "    node_closeness = nx.closeness_centrality(G)\n",
    "    node_av_shortest_paths = {}\n",
    "    for i in range(G.number_of_nodes()):\n",
    "        shortest_paths = nx.algorithms.shortest_paths.generic.shortest_path_length(G, source=i)\n",
    "        average_shortest_path = mean(list(shortest_paths.values())[1:])\n",
    "        node_av_shortest_paths[i] = average_shortest_path\n",
    "    node_metrics = {'Degree': node_degrees, 'Clustering Coefficient': node_clustering_coefficients, 'Betweenness': node_betweenness, \n",
    "                    'Closeness': node_closeness, 'Shortest Path': node_av_shortest_paths}\n",
    "    return node_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1, 2, 3, 4]:\n",
    "    for j in [1, 2, 3, 4, 5]:\n",
    "        with open('../lfr_graphs/mu_0_{0}/graph_0{1}_mu_0_{0}.yml'.format(i, j)) as f:\n",
    "            graph_info = yaml.load(f, Loader=yaml.Loader)\n",
    "        G = graph_info['G']\n",
    "        node_metrics = calc_node_metrics(G)\n",
    "        with open('../lfr_graphs/mu_0_{0}/graph_0{1}_mu_0_{0}_node_features.yml'.format(i, j), 'w') as f:\n",
    "            yaml.dump(node_metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community node metrics\n",
    "\n",
    "Here, I later added additional node metrics based on community membership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_new_metrics():\n",
    "    e_in_list = {i: [] for i in range(200)}\n",
    "    e_out_list = {i: [] for i in range(200)}\n",
    "\n",
    "    e_in_over_e_out = {i: [] for i in range(200)}\n",
    "    odf = {i: [] for i in range(200)}\n",
    "\n",
    "    expansion = {i: [] for i in range(200)}\n",
    "    cut_ratio = {i: [] for i in range(200)}\n",
    "    conductance = {i: [] for i in range(200)}\n",
    "    normalised_cut = {i: [] for i in range(200)}\n",
    "\n",
    "    triangle_participation = {i: [] for i in range(200)}\n",
    "    \n",
    "    new_metric_dict = {'E In': e_in_list, 'E Out': e_out_list, 'E In Over E Out': e_in_over_e_out,\n",
    "                       'ODF': odf, 'Expansion': expansion, 'Cut Ratio': cut_ratio,\n",
    "                       'Conductance': conductance, 'Normalised Cut': normalised_cut, \n",
    "                       'Triangle Participation': triangle_participation}\n",
    "    return new_metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_new_metrics(new_metrics, G, partitions):\n",
    "    for part in partitions:\n",
    "        for comm in part:\n",
    "\n",
    "            comm_subgraph = G.subgraph(comm)\n",
    "            comm_degrees = comm_subgraph.degree()\n",
    "            \n",
    "            w = len(comm)\n",
    "            N = G.number_of_nodes()\n",
    "            m = G.number_of_edges()\n",
    "\n",
    "            # In order to find the triangle participation for all nodes, find all the triangles in a community\n",
    "            all_cliques = nx.enumerate_all_cliques(comm_subgraph)\n",
    "            triangle_cliques = [k for k in all_cliques if len(k) == 3]\n",
    "\n",
    "            for nod in dict(comm_degrees).keys():\n",
    "                e_in = comm_degrees[nod]\n",
    "                e_out = node_degrees[nod] - e_in\n",
    "\n",
    "                new_metrics['E In'][nod].append(e_in)\n",
    "                new_metrics['E Out'][nod].append(e_out)\n",
    "\n",
    "                # For e_in divided by e_out, if e_out is 0, just return the value of e_in\n",
    "                try:\n",
    "                    new_metrics['E In Over E Out'][nod].append(e_in/e_out)\n",
    "                except ZeroDivisionError:\n",
    "                    new_metrics['E In Over E Out'][nod].append(e_in)\n",
    "\n",
    "                new_metrics['ODF'][nod].append(e_out/node_degrees[nod])\n",
    "\n",
    "                new_metrics['Expansion'][nod].append(e_out/w)\n",
    "                new_metrics['Cut Ratio'][nod].append(e_out/(N-w))\n",
    "\n",
    "                ct = e_out/(node_degrees[nod] + e_in)\n",
    "                new_metrics['Conductance'][nod].append(ct)\n",
    "\n",
    "                nc = ct + e_out/(2*m - 2*e_in + e_out)\n",
    "                new_metrics['Normalised Cut'][nod].append(nc)\n",
    "\n",
    "                # Calculate triangle participation\n",
    "                nods_in_triangles = []\n",
    "                for triangle in triangle_cliques:\n",
    "                    if nod in triangle:\n",
    "                        nods_in_triangles += triangle\n",
    "                tp = len(set(nods_in_triangles))/w\n",
    "                new_metrics['Triangle Participation'][nod].append(tp)\n",
    "                \n",
    "    return new_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_metrics(new_metrics):\n",
    "    averaged_metrics = new_metrics.copy()\n",
    "    for met in averaged_metrics.keys():\n",
    "        for nod in averaged_metrics[met].keys():\n",
    "            averaged_metrics[met][nod] = mean(new_metrics[met][nod])\n",
    "    return averaged_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_node_metrics(node_metrics, partitions):\n",
    "    new_metrics = initialise_new_metrics()\n",
    "    new_metrics = calc_new_metrics(new_metrics, G, partitions)\n",
    "    new_metrics = average_metrics(new_metrics)\n",
    "    updated_node_metrics = node_metrics.copy()\n",
    "    updated_node_metrics.update(new_metrics)\n",
    "    return updated_node_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 1/20 [00:26<08:19, 26.32s/it]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 2/20 [00:49<07:35, 25.32s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 3/20 [01:12<06:58, 24.61s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 4/20 [01:34<06:21, 23.82s/it]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 5/20 [01:56<05:50, 23.38s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 6/20 [02:20<05:27, 23.42s/it]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 7/20 [02:41<04:55, 22.69s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 8/20 [03:03<04:30, 22.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 9/20 [03:27<04:11, 22.89s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 10/20 [03:52<03:56, 23.68s/it]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 11/20 [04:13<03:26, 22.95s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 12/20 [04:34<02:58, 22.26s/it]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▌   | 13/20 [04:54<02:32, 21.73s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 14/20 [05:15<02:09, 21.51s/it]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 15/20 [05:36<01:46, 21.31s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 16/20 [05:58<01:25, 21.33s/it]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▌ | 17/20 [06:18<01:03, 21.14s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 18/20 [06:44<00:44, 22.40s/it]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▌| 19/20 [07:05<00:22, 22.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 20/20 [07:30<00:00, 22.54s/it]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "x = [(i,j) for i in [1,2,3,4] for j in [1,2,3,4,5]]\n",
    "\n",
    "for i, j in tqdm(x):\n",
    "    with open('../lfr_graphs/mu_0_{0}/graph_0{1}/graph_0{1}_mu_0_{0}_node_features.yml'.format(i, j)) as f:\n",
    "        node_metrics = yaml.load(f, Loader=yaml.Loader)\n",
    "    with open('../lfr_graphs/mu_0_{0}/graph_0{1}/graph_0{1}_mu_0_{0}_partitions.yml'.format(i, j)) as f:\n",
    "        partitions = yaml.load(f, Loader=yaml.Loader)\n",
    "    node_metrics = new_node_metrics(node_metrics, partitions)\n",
    "    with open('../lfr_graphs/mu_0_{0}/graph_0{1}/graph_0{1}_mu_0_{0}_node_features.yml'.format(i, j), 'w') as f:\n",
    "        yaml.dump(node_metrics, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
